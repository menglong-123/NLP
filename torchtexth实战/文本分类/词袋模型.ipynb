{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = AG_NEWS(root='NLP/dataset/IMDB',\n",
    "                                                split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(dataset):\n",
    "    for label, text in train_dataset:\n",
    "        yield [token for token in tokenizer(text)]\n",
    "    \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=['<unk>', '<pad>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    labels, texts = [], []\n",
    "    for label, text in data:\n",
    "        labels.append(int(label) - 1)\n",
    "        text = torch.tensor(vocab(tokenizer(text)), dtype=torch.long)\n",
    "        texts.append(text)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>'])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, num_classes)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # [batch_size, src_len, emb_dim]\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = torch.mean(embedded,dim=1,keepdim=False)\n",
    "        out = self.fc(embedded)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "num_class  = len(set([label for (label, text) in train_dataset]))\n",
    "print(num_class)\n",
    "vocab_size = len(vocab)\n",
    "emb_dim = 64\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(vocab_size, emb_dim, num_class).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for text, label in train_dataloader:\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            out = model(text)\n",
    "            loss = criterion(out, label)\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('epoch:{}, loss:{}'.format(epoch + 1, epoch_loss / len(list(train_dataloader))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, loss:1.1394813102744559\n",
      "epoch:2, loss:0.6506261299806573\n",
      "epoch:3, loss:0.43451570359276354\n",
      "epoch:4, loss:0.3454782765283147\n",
      "epoch:5, loss:0.3007818692251483\n",
      "epoch:6, loss:0.2681596132200072\n",
      "epoch:7, loss:0.246959580030996\n",
      "epoch:8, loss:0.22800007607219314\n",
      "epoch:9, loss:0.21403573672093348\n",
      "epoch:10, loss:0.20116036626626613\n",
      "epoch:11, loss:0.18931650936508126\n",
      "epoch:12, loss:0.1797315784935763\n",
      "epoch:13, loss:0.1698638417518508\n",
      "epoch:14, loss:0.1612623284631503\n",
      "epoch:15, loss:0.15381027417597357\n",
      "epoch:16, loss:0.1467962062290473\n",
      "epoch:17, loss:0.14031255848530835\n",
      "epoch:18, loss:0.13381127848537397\n",
      "epoch:19, loss:0.12776993468340203\n",
      "epoch:20, loss:0.1220421885580556\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text, label in test_dataloader:\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            out = model(text)\n",
    "            loss = criterion(out, label)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            out = out.argmax(dim=-1)\n",
    "            correct += (out == label).sum()\n",
    "            total += len(label)\n",
    "            \n",
    "        print('loss:{}, acc:{}'.format(epoch_loss / len(list(test_dataloader)), correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.25506291172261963, acc:0.9184321761131287\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
