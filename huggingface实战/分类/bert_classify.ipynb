{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, path) -> None:\n",
    "        super().__init__()\n",
    "        self.pretrained = AutoModel.from_pretrained(path)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        out = self.pretrained(input_ids, attention_mask, token_type_ids)\n",
    "        out = out.last_hidden_state[:, 0]\n",
    "        out = self.dropout(out)\n",
    "        logits = self.fc(out)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"huggingface/models/bert_base_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "dataset = load_from_disk(\"huggingface/datasets/glue/sst2\")\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess_function(data):\n",
    "    data = tokenizer.batch_encode_plus(data['sentence'], truncation=True)\n",
    "    return data\n",
    "# 处理数据集\n",
    "dataset = dataset.map(function=preprocess_function, batched=True, batch_size=1000,\n",
    "                num_proc=4, remove_columns=['sentence', 'idx'])\n",
    "\n",
    "# 获取训练集,验证集和测试集\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=128, \n",
    "        collate_fn=DataCollatorWithPadding(tokenizer),shuffle=True, drop_last=True)\n",
    "\n",
    "valid_dataloader = DataLoader(dataset['validation'], batch_size=128, \n",
    "        collate_fn=DataCollatorWithPadding(tokenizer),shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset['test'], batch_size=64, shuffle=False, drop_last=True,\n",
    "                                collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "model = Model(num_classes=2, path=path)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# 对预训练的Bert和分类头使用不同的学习率\n",
    "optimizer = AdamW([\n",
    "    {\"params\": model.pretrained.parameters(), 'lr': 2e-5},\n",
    "    {\"params\": model.fc.parameters(), 'lr': 5e-4}\n",
    "])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在验证集上的精度\n",
    "def test():\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct, total = 0, 0\n",
    "    for idx, data in enumerate(valid_dataloader):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "        logits = logits.argmax(dim=-1)\n",
    "        correct += (logits == labels).sum()\n",
    "        total += len(labels)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数，使用验证集上的精度来选择模型\n",
    "def train():\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    correct, total = 0, 0\n",
    "    total_loss, updates = 0, 0\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            logits = logits.argmax(dim=-1)\n",
    "            correct += (logits == labels).sum()\n",
    "            total += len(labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            updates += 1\n",
    "\n",
    "            if idx % 50 == 0:\n",
    "                print('epoch:{}, idx:{}, loss:{}, acc:{}'.format(epoch + 1, idx, \n",
    "                                                        total_loss / updates, correct / total))\n",
    "        acc = test()\n",
    "        print(\"epoch:{}, valid_dataset acc:{}\".format(epoch + 1, acc))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'code/huggingface实战/分类/classify.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, idx:0, loss:0.7110611200332642, acc:0.484375\n",
      "epoch:1, idx:50, loss:0.4006512132929821, acc:0.8144914507865906\n",
      "epoch:1, idx:100, loss:0.3311491499442865, acc:0.8524907231330872\n",
      "epoch:1, idx:150, loss:0.30035069584846497, acc:0.8683257699012756\n",
      "epoch:1, idx:200, loss:0.2788397831777435, acc:0.8802083134651184\n",
      "epoch:1, idx:250, loss:0.2653611544297036, acc:0.8882283568382263\n",
      "epoch:1, idx:300, loss:0.25490892091858824, acc:0.8935838937759399\n",
      "epoch:1, idx:350, loss:0.24564787488632392, acc:0.8981258869171143\n",
      "epoch:1, idx:400, loss:0.23697058759871267, acc:0.9024314284324646\n",
      "epoch:1, idx:450, loss:0.2284041431519779, acc:0.9069775342941284\n",
      "epoch:1, idx:500, loss:0.22317740579922044, acc:0.9096494317054749\n",
      "epoch:1, valid_dataset acc:0.9231771230697632\n",
      "epoch:2, idx:0, loss:0.21962794293941085, acc:0.9113348126411438\n",
      "epoch:2, idx:50, loss:0.20998327766562871, acc:0.9159174561500549\n",
      "epoch:2, idx:100, loss:0.2008533557101585, acc:0.9200558066368103\n",
      "epoch:2, idx:150, loss:0.19309104467292304, acc:0.9235252141952515\n",
      "epoch:2, idx:200, loss:0.18672263637656522, acc:0.92643141746521\n",
      "epoch:2, idx:250, loss:0.18119356770217035, acc:0.9289333820343018\n",
      "epoch:2, idx:300, loss:0.17665555451105844, acc:0.9311707019805908\n",
      "epoch:2, idx:350, loss:0.17248293322467478, acc:0.9330102205276489\n",
      "epoch:2, idx:400, loss:0.16932187910556407, acc:0.9344912767410278\n",
      "epoch:2, idx:450, loss:0.1666825518713637, acc:0.9357967376708984\n",
      "epoch:2, idx:500, loss:0.16350562731574897, acc:0.9372565746307373\n",
      "epoch:2, valid_dataset acc:0.91796875\n",
      "epoch:3, idx:0, loss:0.16191442240585005, acc:0.9379154443740845\n",
      "epoch:3, idx:50, loss:0.15693191370436227, acc:0.9400215148925781\n",
      "epoch:3, idx:100, loss:0.1520990927498513, acc:0.9420262575149536\n",
      "epoch:3, idx:150, loss:0.14824536075608083, acc:0.943604588508606\n",
      "epoch:3, idx:200, loss:0.14471242971776288, acc:0.9450506567955017\n",
      "epoch:3, idx:250, loss:0.14146024498878387, acc:0.9464336633682251\n",
      "epoch:3, idx:300, loss:0.13855143242769621, acc:0.9476510286331177\n",
      "epoch:3, idx:350, loss:0.1357107166475029, acc:0.9487760663032532\n",
      "epoch:3, idx:400, loss:0.1331440895467007, acc:0.949834406375885\n",
      "epoch:3, idx:450, loss:0.1310793538336194, acc:0.9506975412368774\n",
      "epoch:3, idx:500, loss:0.12927682691086406, acc:0.9514548778533936\n",
      "epoch:3, valid_dataset acc:0.9153646230697632\n",
      "epoch:4, idx:0, loss:0.1281817772377346, acc:0.9519573450088501\n",
      "epoch:4, idx:50, loss:0.12504889022749308, acc:0.9531537890434265\n",
      "epoch:4, idx:100, loss:0.12234842063561792, acc:0.9542370438575745\n",
      "epoch:4, idx:150, loss:0.1197293037255098, acc:0.9553164839744568\n",
      "epoch:4, idx:200, loss:0.11730188426984126, acc:0.9563044309616089\n",
      "epoch:4, idx:250, loss:0.11506524275934366, acc:0.9571486711502075\n",
      "epoch:4, idx:300, loss:0.11288819158426969, acc:0.9579813480377197\n",
      "epoch:4, idx:350, loss:0.11082813263573371, acc:0.9587950110435486\n",
      "epoch:4, idx:400, loss:0.10896388732403717, acc:0.959536075592041\n",
      "epoch:4, idx:450, loss:0.10735810560838926, acc:0.9601482152938843\n",
      "epoch:4, idx:500, loss:0.10579485500564224, acc:0.9607571363449097\n",
      "epoch:4, valid_dataset acc:0.921875\n",
      "epoch:5, idx:0, loss:0.10491933756676058, acc:0.9611045122146606\n",
      "epoch:5, idx:50, loss:0.1028004116801304, acc:0.9619054198265076\n",
      "epoch:5, idx:100, loss:0.10085485290070466, acc:0.9626381993293762\n",
      "epoch:5, idx:150, loss:0.09908204994239522, acc:0.9633141756057739\n",
      "epoch:5, idx:200, loss:0.09746437724860266, acc:0.9639201164245605\n",
      "epoch:5, idx:250, loss:0.09583327079466405, acc:0.9645335674285889\n",
      "epoch:5, idx:300, loss:0.09426909330650232, acc:0.9651312828063965\n",
      "epoch:5, idx:350, loss:0.09279473432975863, acc:0.9656727313995361\n",
      "epoch:5, idx:400, loss:0.09145558093071027, acc:0.9661613702774048\n",
      "epoch:5, idx:450, loss:0.09022904979054809, acc:0.9666004776954651\n",
      "epoch:5, idx:500, loss:0.08896143835362166, acc:0.9670465588569641\n",
      "epoch:5, valid_dataset acc:0.921875\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理函数\n",
    "def inference():\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for data in test_dataloader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "        logits = logits.argmax(dim=-1)\n",
    "        for input_id in input_ids:\n",
    "            sentence = tokenizer.decode(input_id)\n",
    "            sentence = sentence.replace('[CLS]', '').replace('[SEP]', '').replace('[PAD]', '').strip()\n",
    "            sentences.append(sentence)\n",
    "        labels.extend(logits.detach().cpu().numpy())\n",
    "    res = pd.DataFrame()\n",
    "    res['sentences'] = sentences\n",
    "    res['labels'] = labels\n",
    "\n",
    "    res.to_csv(\"code/huggingface实战/分类/inference.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"code/huggingface实战/分类/classify.pt\"))\n",
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
