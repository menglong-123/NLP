{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import evaluate\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"huggingface/models/opus-mt-de-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    de = [example['translation']['de'] for example in data]\n",
    "    en = [example['translation']['en'] for example in data]\n",
    "    data = tokenizer.batch_encode_plus(de, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        data['labels'] = tokenizer.batch_encode_plus(en, padding=True, truncation=True, max_length=128, return_tensors='pt')['input_ids']\n",
    "\n",
    "    data['decoder_input_ids'] = torch.full_like(data['labels'], tokenizer.get_vocab()['<pad>'])\n",
    "    data['decoder_input_ids'][:,1:] = data['labels'][:,:-1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"huggingface/datasets/wmt16/de-en\")\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=128, shuffle=True, \n",
    "                            drop_last=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(dataset['validation'], batch_size=128, shuffle=True, \n",
    "                            drop_last=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(dataset['test'], batch_size=128, shuffle=True, \n",
    "                            drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(path)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(512, tokenizer.vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids):\n",
    "        out = self.backbone(input_ids, attention_mask, decoder_input_ids)\n",
    "        out = out.last_hidden_state\n",
    "        out = self.fc(self.dropout(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "model = Model()\n",
    "optimizer = AdamW([\n",
    "    {\"params\": model.backbone.parameters(), 'lr': 2e-5},\n",
    "    {\"params\": model.fc.parameters(), 'lr': 5e-4}\n",
    "])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"training\")\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            decoder_input_ids = data['decoder_input_ids'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            \n",
    "            out = model(input_ids, attention_mask, decoder_input_ids)\n",
    "            output_dim = out.shape[-1]\n",
    "            out = out.view(-1, output_dim)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            loss = criterion(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                loss_item = epoch_loss / (i + 1)\n",
    "                print('epoch:{}, idx:{}, loss:{}, PPL:{}'.format(epoch+1, i, loss_item, math.exp(loss_item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(predictions, references):\n",
    "    references = [[i] for i in references]\n",
    "    metric = evaluate.load('bleu')\n",
    "    metric_out = metric.compute(predictions=predictions, references=references)\n",
    "    return metric_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, max_length=128):  # 给定一个德语句子，返回其翻译\n",
    "    data = tokenizer.encode_plus(sentence, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    reference = [tokenizer.get_vocab()['<pad>']]\n",
    "    for i in range(max_length):\n",
    "        decoder_input_ids = torch.tensor(reference).unsqueeze(0).to(device)\n",
    "        out = model(input_ids, attention_mask, decoder_input_ids)\n",
    "        pred_token = out.argmax(dim=-1)[:,-1].item()\n",
    "        if pred_token == 0:\n",
    "            break\n",
    "        reference.append(pred_token)\n",
    "    return tokenizer.decode(reference[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2169/2169 [09:59<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid bleu:  {'bleu': 0.24595684927854625, 'precisions': [0.5760426666666667, 0.3077215586274773, 0.1822562824702038, 0.11327670323683102], 'brevity_penalty': 1.0, 'length_ratio': 1.009301724694787, 'translation_length': 46875, 'reference_length': 46443}\n"
     ]
    }
   ],
   "source": [
    "valid_references, valid_predictions = [], []\n",
    "for item in tqdm(dataset['validation']['translation']):\n",
    "    pred = translate(item['de'])\n",
    "    valid_predictions.append(pred)\n",
    "    valid_references.append(item['en'])\n",
    "valid_bleu = compute_bleu(valid_predictions, valid_references)\n",
    "print('valid bleu: ', valid_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2999/2999 [14:05<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test bleu:  {'bleu': 0.27688030895299387, 'precisions': [0.5984797271921523, 0.33883400453058143, 0.21203916194753092, 0.13668339962828507], 'brevity_penalty': 1.0, 'length_ratio': 1.038179010901605, 'translation_length': 66567, 'reference_length': 64119}\n"
     ]
    }
   ],
   "source": [
    "test_references, test_predictions = [], []\n",
    "for item in tqdm(dataset['test']['translation']):\n",
    "    pred = translate(item['de'])\n",
    "    test_predictions.append(pred)\n",
    "    test_references.append(item['en'])\n",
    "test_bleu = compute_bleu(test_predictions, test_references)\n",
    "print('test bleu: ', test_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_forcing(model, dataloader):  # 上帝视角,在翻译过程中使用teacher-forcing的方式\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for data in dataloader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        decoder_input_ids = data['decoder_input_ids'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        \n",
    "        out = model(input_ids, attention_mask, decoder_input_ids)\n",
    "        pred = tokenizer.batch_decode(out.argmax(dim=2), skip_special_tokens=True)\n",
    "        label = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        predictions.extend(pred)\n",
    "        references.extend(label)\n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "predictions, references = teacher_forcing(model, valid_dataloader)\n",
    "valid_bleu_x = compute_bleu(predictions, references)\n",
    "predictions_, references_ = teacher_forcing(model, test_dataloader)\n",
    "test_bleu_x = compute_bleu(predictions_, references_)\n",
    "print('valid bleu: ', valid_bleu_x)\n",
    "print('test bleu: ', valid_bleu_x)\n",
    "\n",
    "'''\n",
    "valid bleu:  {'bleu': 0.318229880337491, 'precisions': [0.63511027324142, 0.3905524190666762, 0.25104550121453434, 0.16469594594594594], 'brevity_penalty': 1.0, 'length_ratio': 1.0024818980827908, 'translation_length': 44027, 'reference_length': 43918}\n",
    "test bleu:  {'bleu': 0.318229880337491, 'precisions': [0.63511027324142, 0.3905524190666762, 0.25104550121453434, 0.16469594594594594], 'brevity_penalty': 1.0, 'length_ratio': 1.0024818980827908, 'translation_length': 44027, 'reference_length': 43918}\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
